
@article{hajgato_deep_2020,
	title = {Deep {Reinforcement} {Learning} for {Real}-{Time} {Optimization} of {Pumps} in {Water} {Distribution} {Systems}},
	volume = {146},
	issn = {0733-9496, 1943-5452},
	OPT_URL = {http://arxiv.org/abs/2010.06460},
	doi = {10.1061/(ASCE)WR.1943-5452.0001287},
	abstract = {Real-time control of pumps can be an infeasible task in water distribution systems (WDSs) because the calculation to find the optimal pump speeds is resource-intensive. The computational need cannot be lowered even with the capabilities of smart water networks when conventional optimization techniques are used. Deep reinforcement learning (DRL) is presented here as a controller of pumps in two WDSs. An agent based on a dueling deep q-network is trained to maintain the pump speeds based on instantaneous nodal pressure data. General optimization techniques (e.g., Nelder-Mead method, differential evolution) serve as baselines. The total efficiency achieved by the DRL agent compared to the best performing baseline is above 0.98, whereas the speedup is around 2x compared to that. The main contribution of the presented approach is that the agent can run the pumps in real-time because it depends only on measurement data. If the WDS is replaced with a hydraulic simulation, the agent still outperforms conventional techniques in search speed.},
	number = {11},
	OPT_URLdate = {2024-05-31},
	journal = {Journal of Water Resources Planning and Management},
	author = {Hajgató, Gergely and Paál, György and Gyires-Tóth, Bálint},
	month = nov,
	year = {2020},
	pages = {04020079},
	file = {Hajgató et al. - 2020 - Deep Reinforcement Learning for Real-Time Optimization of Pumps in Water Distribution Systems.pdf:/Users/paulstahlhofen/Zotero/storage/7AILJIMS/Hajgató et al. - 2020 - Deep Reinforcement Learning for Real-Time Optimization of Pumps in Water Distribution Systems.pdf:application/pdf},
}

@article{xu_zone_2021,
	title = {Zone scheduling optimization of pumps in water distribution networks with deep reinforcement learning and knowledge-assisted learning},
	volume = {25},
	issn = {1432-7643, 1433-7479},
	OPT_URL = {https://link.springer.com/10.1007/s00500-021-06177-3},
	doi = {10.1007/s00500-021-06177-3},
	language = {en},
	number = {23},
	OPT_URLdate = {2024-10-07},
	journal = {Soft Computing},
	author = {Xu, Jiahui and Wang, Hongyuan and Rao, Jun and Wang, Jingcheng},
	month = dec,
	year = {2021},
	pages = {14757--14767},
	file = {Xu et al. - 2021 - Zone scheduling optimization of pumps in water dis.pdf:/Users/paulstahlhofen/Zotero/storage/QWWJU4AS/Xu et al. - 2021 - Zone scheduling optimization of pumps in water dis.pdf:application/pdf},
}

@inproceedings{zaman_optimizing_2023,
	address = {Boca Raton, FL, USA},
	title = {Optimizing {Smart} {City} {Water} {Distribution} {Systems} {Using} {Deep} {Reinforcement} {Learning}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350331110},
	OPT_URL = {https://ieeexplore.ieee.org/document/10374894/},
	doi = {10.1109/HONET59747.2023.10374894},
	OPT_URLdate = {2024-10-09},
	booktitle = {2023 {IEEE} 20th {International} {Conference} on {Smart} {Communities}: {Improving} {Quality} of {Life} using {AI}, {Robotics} and {IoT} ({HONET})},
	publisher = {IEEE},
	author = {Zaman, Mostafa and Tantawy, Ashraf and Abdelwahed, Sherif},
	month = dec,
	year = {2023},
	pages = {228--233},
	file = {Zaman et al. - 2023 - Optimizing Smart City Water Distribution Systems U.pdf:/Users/paulstahlhofen/Zotero/storage/9GNK4T24/Zaman et al. - 2023 - Optimizing Smart City Water Distribution Systems U.pdf:application/pdf},
}

@article{belfadil_leveraging_2024,
	title = {Leveraging {Deep} {Reinforcement} {Learning} for {Water} {Distribution} {Systems} with {Large} {Action} {Spaces} and {Uncertainties}: {DRL}-{EPANET} for {Pressure} {Control}},
	volume = {150},
	issn = {0733-9496, 1943-5452},
	shorttitle = {Leveraging {Deep} {Reinforcement} {Learning} for {Water} {Distribution} {Systems} with {Large} {Action} {Spaces} and {Uncertainties}},
	OPT_URL = {https://ascelibrary.org/doi/10.1061/JWRMD5.WRENG-6108},
	doi = {10.1061/JWRMD5.WRENG-6108},
	abstract = {Deep reinforcement learning (DRL) has undergone a revolution in recent years, enabling researchers to tackle a variety of previously inaccessible sequential decision problems. However, its application to the control of water distribution systems (WDS) remains limited. This research demonstrates the successful application of DRL for pressure control in WDS by simulating an environment using EPANET version 2.2, a popular open-source hydraulic simulator. We highlight the ability of DRL-EPANET to handle large action spaces, with more than 1 million possible actions in each time step, and its capacity to deal with uncertainties such as random pipe breaks. We employ the Branching Dueling Q-Network (BDQ) algorithm, which can learn in this context, and enhance it with an algorithmic modification called BDQ with fixed actions (BDQF) that achieves better rewards, especially when manipulated actions are sparse. The proposed methodology was validated using the hydraulic models of 10 real WDS, one of which integrated transmission and distribution systems operated by Hidralia, and the rest of which were operated by Aigües de Barcelona. DOI: 10.1061/JWRMD5.WRENG-6108. © 2023 American Society of Civil Engineers.},
	language = {en},
	number = {2},
	OPT_URLdate = {2024-10-23},
	journal = {Journal of Water Resources Planning and Management},
	author = {Belfadil, Anas and Modesto, David and Meseguer, Jordi and Joseph-Duran, Bernat and Saporta, David and Martin Hernandez, Jose Antonio},
	month = feb,
	year = {2024},
	pages = {04023076},
	file = {Belfadil et al. - 2024 - Leveraging Deep Reinforcement Learning for Water D.pdf:/Users/paulstahlhofen/Zotero/storage/9W36LTBT/Belfadil et al. - 2024 - Leveraging Deep Reinforcement Learning for Water D.pdf:application/pdf},
}

@article{hu_real-time_2023,
	title = {Real-{Time} {Scheduling} of {Pumps} in {Water} {Distribution} {Systems} {Based} on {Exploration}-{Enhanced} {Deep} {Reinforcement} {Learning}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2079-8954},
	OPT_URL = {https://www.mdpi.com/2079-8954/11/2/56},
	doi = {10.3390/systems11020056},
	abstract = {Effective ways to optimise real-time pump scheduling to maximise energy efficiency are being sought to meet the challenges in the energy market. However, the considerable number of evaluations of popular optimisation methods based on metaheuristics cause significant delays for real-time pump scheduling, and the simplification of traditional deterministic methods may introduce bias towards the optimal solutions. To address these limitations, an exploration-enhanced deep reinforcement learning (DRL) framework is proposed to address real-time pump scheduling problems in water distribution systems. The experimental results indicate that E-PPO can learn suboptimal scheduling policies for various demand distributions and can control the application time to 0.42 s by transferring the online computation-intensive optimisation task offline. Furthermore, a form of penalty of the tank level was found that can reduce energy costs by up to 11.14\% without sacrificing the water level in the long term. Following the DRL framework, the proposed method makes it possible to schedule pumps in a more agile way as a timely response to changing water demand while still controlling the energy cost and level of tanks.},
	language = {en},
	number = {2},
	OPT_URLdate = {2024-11-05},
	journal = {Systems},
	author = {Hu, Shiyuan and Gao, Jinliang and Zhong, Dan and Wu, Rui and Liu, Luming},
	month = jan,
	year = {2023},
	pages = {56},
	file = {Hu et al. - 2023 - Real-Time Scheduling of Pumps in Water Distributio.pdf:/Users/paulstahlhofen/Zotero/storage/KEQXZ4GI/Hu et al. - 2023 - Real-Time Scheduling of Pumps in Water Distributio.pdf:application/pdf},
}

@article{martinez-piazuelo_multi-critic_2020,
	title = {A {Multi}-{Critic} {Reinforcement} {Learning} {Method}: {An} {Application} to {Multi}-{Tank} {Water} {Systems}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	shorttitle = {A {Multi}-{Critic} {Reinforcement} {Learning} {Method}},
	OPT_URL = {https://ieeexplore.ieee.org/document/9200594/},
	doi = {10.1109/ACCESS.2020.3025194},
	OPT_URLdate = {2024-12-10},
	journal = {IEEE Access},
	author = {Martinez-Piazuelo, Juan and Ochoa, Daniel E. and Quijano, Nicanor and Giraldo, Luis Felipe},
	year = {2020},
	pages = {173227--173238},
	file = {Martinez-Piazuelo_2020_A-Multi-Critic-Reinforcement-Learning-Method-An-Application-to-Multi-Tank-Water-Systems.pdf:/Users/paulstahlhofen/Zotero/storage/R8IIX343/Martinez-Piazuelo_2020_A-Multi-Critic-Reinforcement-Learning-Method-An-Application-to-Multi-Tank-Water-Systems.pdf:application/pdf},
}

@inproceedings{val_safe_2021,
	address = {San Diego, CA, USA},
	title = {Safe {Reinforcement} {Learning} {Control} for {Water} {Distribution} {Networks}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66543-643-4},
	OPT_URL = {https://ieeexplore.ieee.org/document/9659138/},
	doi = {10.1109/CCTA48906.2021.9659138},
	OPT_URLdate = {2024-12-10},
	booktitle = {2021 {IEEE} {Conference} on {Control} {Technology} and {Applications} ({CCTA})},
	publisher = {IEEE},
	author = {Val, Jorge and Wisniewski, Rafal and Kallesoe, Carsten Skovmose},
	month = aug,
	year = {2021},
	pages = {1148--1153},
	file = {Val_2021_Safe-Reinforcement-Learning-Control-for-Water-Distribution-Networks.pdf:/Users/paulstahlhofen/Zotero/storage/NGEAJIZS/Val_2021_Safe-Reinforcement-Learning-Control-for-Water-Distribution-Networks.pdf:application/pdf},
}

@article{hu_multi-objective_2022,
	title = {Multi-objective deep reinforcement learning for emergency scheduling in a water distribution network},
	volume = {14},
	issn = {1865-9284, 1865-9292},
	OPT_URL = {https://link.springer.com/10.1007/s12293-022-00366-9},
	doi = {10.1007/s12293-022-00366-9},
	language = {en},
	number = {2},
	OPT_URLdate = {2024-12-10},
	journal = {Memetic Computing},
	author = {Hu, Chengyu and Wang, Qiuming and Gong, Wenyin and Yan, Xuesong},
	month = jun,
	year = {2022},
	pages = {211--223},
	file = {Hu et al. - 2022 - Multi-objective deep reinforcement learning for em.pdf:/Users/paulstahlhofen/Zotero/storage/T2KRITRB/Hu et al. - 2022 - Multi-objective deep reinforcement learning for em.pdf:application/pdf},
}

@article{fan_graph_2022,
	title = {A graph convolution network‐deep reinforcement learning model for resilient water distribution network repair decisions},
	volume = {37},
	issn = {1093-9687, 1467-8667},
	OPT_URL = {https://onlinelibrary.wiley.com/doi/10.1111/mice.12813},
	doi = {10.1111/mice.12813},
	abstract = {Abstract
            Water distribution networks (WDNs) are critical infrastructure for communities. The dramatic expansion of the WDNs associated with urbanization makes them more vulnerable to high‐consequence hazards such as earthquakes, which requires strategies to ensure their resilience. The resilience of a WDN is related to its ability to recover its service after disastrous events. Sound decisions on the repair sequence play a crucial role to ensure a resilient WDN recovery. This paper introduces the development of a graph convolutional neural network‐integrated deep reinforcement learning (GCN‐DRL) model to support optimal repair decisions to improve WDN resilience after earthquakes. A WDN resilience evaluation framework is first developed, which integrates the dynamic evolution of WDN performance indicators during the post‐earthquake recovery process. The WDN performance indicator considers the relative importance of the service nodes and the extent of post‐earthquake water needs that are satisfied. In this GCN‐DRL model framework, the GCN encodes the information of the WDN. The topology and performance of service nodes (i.e., the degree of water that needs satisfaction) are inputs to the GCN; the outputs of GCN are the reward values (Q‐values) corresponding to each repair action, which are fed into the DRL process to select the optimal repair sequence from a large action space to achieve highest system resilience. The GCN‐DRL model is demonstrated on a testbed WDN subjected to three earthquake damage scenarios. The performance of the repair decisions by the GCN‐DRL model is compared with those by four conventional decision methods. The results show that the recovery sequence by the GCN‐DRL model achieved the highest system resilience index values and the fastest recovery of system performance. Besides, by using transfer learning based on a pre‐trained model, the GCN‐DRL model achieved high computational efficiency in determining the optimal repair sequences under new damage scenarios. This novel GCN‐DRL model features robustness and universality to support optimal repair decisions to ensure resilient WDN recovery from earthquake damages.},
	language = {en},
	number = {12},
	OPT_URLdate = {2024-12-10},
	journal = {Computer-Aided Civil and Infrastructure Engineering},
	author = {Fan, Xudong and Zhang, Xijin and Yu, Xiong (Bill)},
	month = oct,
	year = {2022},
	pages = {1547--1565},
	file = {Fan et al. - 2022 - A graph convolution network‐deep reinforcement lea.pdf:/Users/paulstahlhofen/Zotero/storage/UGXY5RMK/Fan et al. - 2022 - A graph convolution network‐deep reinforcement lea.pdf:application/pdf},
}

@incollection{candelieri_intelligent_2019,
	address = {Cham},
	title = {Intelligent {Pump} {Scheduling} {Optimization} in {Water} {Distribution} {Networks}},
	volume = {11353},
	isbn = {978-3-030-05348-2},
	OPT_URL = {https://link.springer.com/10.1007/978-3-030-05348-2_30},
	language = {en},
	OPT_URLdate = {2024-12-10},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Candelieri, Antonio and Perego, Riccardo and Archetti, Francesco},
	editor = {Battiti, Roberto and Brunato, Mauro and Kotsireas, Ilias and Pardalos, Panos M.},
	year = {2019},
	doi = {10.1007/978-3-030-05348-2_30},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {352--369},
	file = {Candelieri et al. - 2019 - Intelligent Pump Scheduling Optimization in Water .pdf:/Users/paulstahlhofen/Zotero/storage/QJWCHGQY/Candelieri et al. - 2019 - Intelligent Pump Scheduling Optimization in Water .pdf:application/pdf},
}

@misc{donancio_pump_2022,
	title = {The {Pump} {Scheduling} {Problem}: {A} {Real}-{World} {Scenario} for {Reinforcement} {Learning}},
	shorttitle = {The {Pump} {Scheduling} {Problem}},
	OPT_URL = {http://arxiv.org/abs/2210.11111},
	doi = {10.48550/arXiv.2210.11111},
	abstract = {Deep Reinforcement Learning (DRL) has achieved remarkable success in scenarios such as games and has emerged as a potential solution for control tasks. That is due to its ability to leverage scalability and handle complex dynamics. However, few works have targeted environments grounded in real-world settings. Indeed, real-world scenarios can be challenging, especially when faced with the high dimensionality of the state space and unknown reward function. We release a testbed consisting of an environment simulator and demonstrations of human operation concerning pump scheduling of a real-world water distribution facility to facilitate research. The pump scheduling problem can be viewed as a decision process to decide when to operate pumps to supply water while limiting electricity consumption and meeting system constraints. To provide a starting point, we release a well-documented codebase, present an overview of some challenges that can be addressed and provide a baseline representation of the problem. The code and dataset are available at https://gitlab.com/hdonancio/pumpscheduling.},
	OPT_URLdate = {2024-12-10},
	author = {Donâncio, Henrique and Vercouter, Laurent and Roclawski, Harald},
	month = oct,
	year = {2022},
	note = {arXiv:2210.11111 [cs]},
	file = {Donâncio et al. - 2022 - The Pump Scheduling Problem A Real-World Scenario for Reinforcement Learning.pdf:/Users/paulstahlhofen/Zotero/storage/W9K6FHLM/Donâncio et al. - 2022 - The Pump Scheduling Problem A Real-World Scenario for Reinforcement Learning.pdf:application/pdf},
}

@article{choi_development_2020,
	title = {Development of {Cross}-{Domain} {Artificial} {Neural} {Network} to {Predict} {High}-{Temporal} {Resolution} {Pressure} {Data}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2071-1050},
	OPT_URL = {https://www.mdpi.com/2071-1050/12/9/3832},
	doi = {10.3390/su12093832},
	abstract = {Forecasting hydraulic data such as pressure and demand in water distribution system (WDS) is an important task that helps ensure efficient and accurate operations. Despite high-performance data prediction, missing data can still occur, making it difficult to effectively operate WDS. Though the pressure data are directly related to the rules of operation for pumps or valves, few studies have been conducted on pressure data forecasting. This study proposes a new missing and incomplete data control approach based on real pressure data for reliable and efficient WDS operation and maintenance. The proposed approach is: (1) application of source data from high-resolution, real-world pressure data; (2) development of a cross-domain artificial neural network (CDANN), combining the standard artificial neural networks (ANNs) and the cross-domain training approach for missing data control; and (3) analysis of standard data mining according to external factors to improve prediction accuracy. To verify the proposed approach, a real-world network located in South Korea was used, and the forecasting results were evaluated through performance indicators (i.e., overall, special points, and percentage errors). The performance of the CDANN is compared with that of standard ANNs, and CDANN was found to provide better predictions than traditional ANNs.},
	language = {en},
	number = {9},
	OPT_URLdate = {2024-12-11},
	journal = {Sustainability},
	author = {Choi, Young Hwan and Jung, Donghwi},
	month = may,
	year = {2020},
	pages = {3832},
	file = {Choi und Jung - 2020 - Development of Cross-Domain Artificial Neural Netw.pdf:/Users/paulstahlhofen/Zotero/storage/HPQ4BQZV/Choi und Jung - 2020 - Development of Cross-Domain Artificial Neural Netw.pdf:application/pdf},
}

@inproceedings{donancio_safety_2022,
	title = {Safety through intrinsically motivated imitation learning},
	OPT_URL = {https://hal.science/hal-03765564/},
	booktitle = {20èmes {Rencontres} des {Jeunes} {Chercheurs} en {Intelligence} {Artificielle}},
	author = {Donancio, Henrique and Vercouter, Laurent},
	year = {2022},
	file = {Donancio und Vercouter - 2022 - Safety through intrinsically motivated imitation l.pdf:/Users/paulstahlhofen/Zotero/storage/N73CF35D/Donancio und Vercouter - 2022 - Safety through intrinsically motivated imitation l.pdf:application/pdf},
}

@article{johansson_quadruple-tank_2000,
	title = {The quadruple-tank process: a multivariable laboratory process with an adjustable zero},
	volume = {8},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {10636536},
	shorttitle = {The quadruple-tank process},
	OPT_URL = {http://ieeexplore.ieee.org/document/845876/},
	doi = {10.1109/87.845876},
	number = {3},
	OPT_URLdate = {2024-06-03},
	journal = {IEEE Transactions on Control Systems Technology},
	author = {Johansson, K.H.},
	month = may,
	year = {2000},
	pages = {456--465},
	file = {FULLTEXT01.pdf:/Users/paulstahlhofen/Zotero/storage/AC7R6YTY/FULLTEXT01.pdf:application/pdf},
}

@article{reis_cost-efficient_2024,
	title = {Cost-{Efficient} {Pump} {Operation} in {Water} {Supply} {Systems} {Considering} {Demand}-{Side} {Management}},
	volume = {150},
	issn = {0733-9496, 1943-5452},
	OPT_URL = {https://ascelibrary.org/doi/10.1061/JWRMD5.WRENG-6215},
	doi = {10.1061/JWRMD5.WRENG-6215},
	language = {en},
	number = {6},
	OPT_URLdate = {2024-11-14},
	journal = {Journal of Water Resources Planning and Management},
	author = {Reis, Ana L. and Andrade-Campos, A. and Henggeler Antunes, Carlos and Lopes, Marta A. R. and Antunes, André},
	month = jun,
	year = {2024},
	pages = {04024017},
	file = {Reis et al. - 2024 - Cost-Efficient Pump Operation in Water Supply Syst.pdf:/Users/paulstahlhofen/Zotero/storage/RPS5VUXS/Reis et al. - 2024 - Cost-Efficient Pump Operation in Water Supply Syst.pdf:application/pdf},
}

@article{predescu_advanced_2020,
	title = {An {Advanced} {Learning}-{Based} {Multiple} {Model} {Control} {Supervisor} for {Pumping} {Stations} in a {Smart} {Water} {Distribution} {System}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2227-7390},
	OPT_URL = {https://www.mdpi.com/2227-7390/8/6/887},
	doi = {10.3390/math8060887},
	abstract = {Water distribution is fundamental to modern society, and there are many associated challenges in the context of large metropolitan areas. A multi-domain approach is required for designing modern solutions for the existing infrastructure, including control and monitoring systems, data science and Machine Learning. Considering the large scale water distribution networks in metropolitan areas, machine and deep learning algorithms can provide improved adaptability for control applications. This paper presents a monitoring and control machine learning-based architecture for a smart water distribution system. Automated test scenarios and learning methods are proposed and designed to predict the network configuration for a modern implementation of a multiple model control supervisor with increased adaptability to changing operating conditions. The high-level processing and components for smart water distribution systems are supported by the smart meters, providing real-time data, push-based and decoupled software architectures and reactive programming.},
	language = {en},
	number = {6},
	OPT_URLdate = {2024-12-10},
	journal = {Mathematics},
	author = {Predescu, Alexandru and Truică, Ciprian-Octavian and Apostol, Elena-Simona and Mocanu, Mariana and Lupu, Ciprian},
	month = jun,
	year = {2020},
	pages = {887},
	file = {Predescu et al. - 2020 - An Advanced Learning-Based Multiple Model Control .pdf:/Users/paulstahlhofen/Zotero/storage/72M2LW24/Predescu et al. - 2020 - An Advanced Learning-Based Multiple Model Control .pdf:application/pdf},
}

@article{ferrari_optimizing_2023,
	title = {Optimizing {Water} {Distribution} through {Explainable} {AI} and {Rule}-{Based} {Control}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-431X},
	OPT_URL = {https://www.mdpi.com/2073-431X/12/6/123},
	doi = {10.3390/computers12060123},
	abstract = {Optimizing water distribution both from an energy-saving perspective and from a quality of service perspective is a challenging task since it involves a complex system with many nodes, many hidden variables and many operational constraints. For this reason, water distribution systems need to handle a delicate trade-off between the effectiveness and computational time of the solution. In this paper, we propose a new computationally efficient method, named rule-based control, to optimize water distribution networks without the need for a rigorous formulation of the optimization problem. As a matter of fact, since it is based on a machine learning approach, the proposed method employs only a set of historical data, where the configuration can be labeled according to a quality criterion. Since it is a data-driven approach, it could be applied to any complex network where historical labeled data are available. In particular, rule-based control exploits a rule-based classification method that allows us to retrieve the rules leading to good or bad performances of the system, even without any information about its physical laws. The evaluation of the results on some simulated scenarios shows that the proposed approach is able to reduce energy consumption while ensuring a good quality of the service. The proposed approach is currently used in the water distribution system of the Milan (Italy) water main.},
	language = {en},
	number = {6},
	OPT_URLdate = {2025-01-05},
	journal = {Computers},
	author = {Ferrari, Enrico and Verda, Damiano and Pinna, Nicolò and Muselli, Marco},
	month = jun,
	year = {2023},
	keywords = {classification, optimization, rule-based control, water distribution network},
	pages = {123},
}

@inproceedings{val_reinforcement_2021,
	title = {Reinforcement {Learning} {Control} for {Water} {Distribution} {Networks} with {Periodic} {Disturbances}},
	OPT_URL = {https://ieeexplore.ieee.org/abstract/document/9482787},
	doi = {10.23919/ACC50511.2021.9482787},
	abstract = {Cost efficient management of Water Distribution Networks with storage units requires of extensive knowledge of the water network. However, the network models are not always available or the calibration costs are too high for most of small water utilities. This paper proposes a model-free control solution based on Q-learning methods that provides a policy for the operation of the network. This supervisory controller must guarantee the water supply despite of the uncertainty of the daily water consumption and reduce the operation cost. The function approximation proposed for the Q-learning controller uses Fourier Basis Functions which provide an accurate approximation of the periodic disturbances. This paper presents results of the control validation in a simulation framework as well as experimental evidence of the advantages and limitations of the proposed design.},
	OPT_URLdate = {2024-12-28},
	booktitle = {2021 {American} {Control} {Conference} ({ACC})},
	author = {Val, Jorge and Wisniewski, Rafał and Kallesøe, Carsten Skovmose},
	month = may,
	year = {2021},
	note = {ISSN: 2378-5861},
	keywords = {Uncertainty, Neural networks, Distribution networks, Knowledge engineering, Linear systems, Reinforcement learning, Stochastic processes},
	pages = {1010--1015},
	file = {Val et al. - 2021 - Reinforcement Learning Control for Water Distribut.pdf:/Users/paulstahlhofen/Zotero/storage/W8KUHPLA/Val et al. - 2021 - Reinforcement Learning Control for Water Distribut.pdf:application/pdf},
}

@misc{patel_hybrid_2023,
	title = {Hybrid {Reinforcement} {Learning} for {Optimizing} {Pump} {Sustainability} in {Real}-{World} {Water} {Distribution} {Networks}},
	OPT_URL = {http://arxiv.org/abs/2310.09412},
	doi = {10.48550/arXiv.2310.09412},
	abstract = {This article addresses the pump-scheduling optimization problem to enhance real-time control of real-world water distribution networks (WDNs). Our primary objectives are to adhere to physical operational constraints while reducing energy consumption and operational costs. Traditional optimization techniques, such as evolution-based and genetic algorithms, often fall short due to their lack of convergence guarantees. Conversely, reinforcement learning (RL) stands out for its adaptability to uncertainties and reduced inference time, enabling real-time responsiveness. However, the effective implementation of RL is contingent on building accurate simulation models for WDNs, and prior applications have been limited by errors in simulation training data. These errors can potentially cause the RL agent to learn misleading patterns and actions and recommend suboptimal operational strategies. To overcome these challenges, we present an improved "hybrid RL" methodology. This method integrates the benefits of RL while anchoring it in historical data, which serves as a baseline to incrementally introduce optimal control recommendations. By leveraging operational data as a foundation for the agent's actions, we enhance the explainability of the agent's actions, foster more robust recommendations, and minimize error. Our findings demonstrate that the hybrid RL agent can significantly improve sustainability, operational efficiency, and dynamically adapt to emerging scenarios in real-world WDNs.},
	OPT_URLdate = {2024-12-28},
	publisher = {arXiv},
	author = {Patel, Harsh and Zhou, Yuan and Lamb, Alexander P. and Wang, Shu and Luo, Jieliang},
	month = oct,
	year = {2023},
	note = {arXiv:2310.09412},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Patel et al. - 2023 - Hybrid Reinforcement Learning for Optimizing Pump .pdf:/Users/paulstahlhofen/Zotero/storage/S2VC5VVY/Patel et al. - 2023 - Hybrid Reinforcement Learning for Optimizing Pump .pdf:application/pdf},
}

@article{ma_pump_2024,
	title = {Pump {Scheduling} {Optimization} in {Urban} {Water} {Supply} {Stations}: {A} {Physics}‐{Informed} {Multiagent} {Deep} {Reinforcement} {Learning} {Approach}},
	volume = {2024},
	issn = {0363-907X, 1099-114X},
	shorttitle = {Pump {Scheduling} {Optimization} in {Urban} {Water} {Supply} {Stations}},
	OPT_URL = {https://onlinelibrary.wiley.com/doi/10.1155/2024/9557596},
	doi = {10.1155/2024/9557596},
	abstract = {In the urban water supply system, a significant proportion of energy consumption is attributed to the water supply pumping station (WSPS). The conventional manual scheduling method employed by water supply enterprises imposes a considerable economic burden. In this paper, we intend to minimize the energy cost of WSPS by dynamically adjusting the combination of pumps and their operational states while considering the pressure difference of the main pipe and switching times of pump group. Achieving this goal is challenging due to the lack of accurate mechanistic models of pumps, uncertainty in environmental parameters, and temporal coupling constraints in the database. Consequently, a WSPS pump scheduling algorithm based on physics‐informed long short‐term memory (PI‐LSTM) surrogate model and multiagent deep deterministic policy gradient (MADDPG) is proposed. The proposed algorithm operates without prior knowledge of an accurate mechanistic model of the pump units. Combining data‐driven with the physical laws of fluid mechanics improves the prediction accuracy of the model compared to traditional data‐based deep learning models, especially when the amount of data is small. Simulation results based on real‐world trajectories show that the proposed algorithm can reduce energy consumption by 13.38\% compared with the original scheduling scheme. This study highlights the potential of integrating physics‐informed deep learning and reinforcement learning to optimize energy consumption in urban water supply systems.},
	language = {en},
	number = {1},
	OPT_URLdate = {2024-12-28},
	journal = {International Journal of Energy Research},
	author = {Ma, Haixiang and Wang, Xuechun and Wang, Dongsheng},
	editor = {Guan, Weixin},
	month = jan,
	year = {2024},
	pages = {9557596},
	file = {Ma et al. - 2024 - Pump Scheduling Optimization in Urban Water Supply.pdf:/Users/paulstahlhofen/Zotero/storage/9RQXFYDE/Ma et al. - 2024 - Pump Scheduling Optimization in Urban Water Supply.pdf:application/pdf},
}

@article{noauthor_multi-agent_nodate,
	title = {Multi-agent reinforcement learning framework for real-time scheduling of pump and valve in water distribution networks},
	OPT_URL = {https://watermark.silverchair.com/ws023072833.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAABBMwggQPBgkqhkiG9w0BBwagggQAMIID_AIBADCCA_UGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMouU9Ve5Dt3AQWdgQAgEQgIIDxrR1e89eDeAm8IRGxeDoKCKfNY_EB1z6xKlExoOkf2zT-d5FKn1UZnKTgmDlxoJLH6A0PprnEybnDZgFNQEYykSI8C3fPQOPJJhEukwWPTA5UNw27ghNawTKH8iyS9yA41mYMZKS-qosLZXidzgnV_RIO-68j4Cy4l1dcmcWHcJb5ImlShZQRN7msyzZALhtC8VHxx_edHuLXwz99RBQd9PJPCyARykE6OUXox8AK_aAeM1dMbR1x9lVBRu5FT8n5ccSr5Ufj0-V0qwEZsyId38AV5s51fGpi9XXLVA761vDPCl7Wd30zZ5IAM5tEilx1IkFYCaI_G1um3B1G9dHuW-iWr1Lc4huwGE4a0iKuu_F0jbwBWuso6s9RGluYrs49ubMrkLAYjlnXSIcJIE_ZH2Rs8o1rxPc1r9CZTHNOTrdSEzIXj2ge7pksAjIaxkxLN9hQa3WaJt7axHAGJgW8LvCa7lGBYCQVAYL2bsPGHMNcej8fbOsvUx2ncFowWt39p9VxgrfBPmhQwWkmnkvEShlbMNIQaQM4JcwqtO65pNbZGZgkHqLE7YI510cvpUEJMfQPyMUaAfnBAykM3mvGbI7aNf_NCDBsVYuFY2jGP4tc4FGLlQhOgeRc-Ae0PFSpI9HkwmCqIeIO2RuDSzIWi5UW9T7MrVIQUVvqhzFVIDfKPizDmgR-AQftQvGT-93qgq5gSq5B3kUTWcVVthNtYEf9ERosQnITf6sGLPChiiPlTUpHPwsFb9fQ1c3Y1aehho1teFsPC9cSxIxW3xz8_Mg5dIGK3H7SgaAEa4HMtttHmzIvWyZr8AUOkwWxEyVssNqgLao4N9fV6dMVPIsXccf9Th9J7x0jjyxeJGc1GW4ztHCIF6lSuOYBDvX_hiSt4ReZ-gX7JrBGuhiy2k0CMNs9Oxvd45ScvL2kAOnF1StrfQaZ__34GkAyfwi-RLS4hksc5tV1qsKFP0k5MUABjjnIqK6S9G_ZxCYGlrVu5jnGWxUUYN3qT3vuyGTd-wXMbeiaGvGUy1SRt1sRzHN7-lFBdXWZYwfKLZzDGkU3cHveH9huS9QM5IZ7UJm2J_kqitGvW4Tan0j5hmPImWT1hguu9fosHJzBB4kEx35t2B8q_ymUxqstKa6IFpNENFuS7eRDhxXP-RQvmC8M5iWnkVbFDgTZNXD6a90jjeSNZfEF65q7zJERt-t3whvtdf7r_Kp25_CYNZcirXoZvvQ2mUDjotc4nA8ACWTAkwrpEiyoPe8LoH5qMBM8tzvzSOUMhnWj1KfQg},
}

@article{val_ledesma_water_2024,
	title = {Water {Age} {Control} for {Water} {Distribution} {Networks} via {Safe} {Reinforcement} {Learning}},
	volume = {32},
	issn = {1558-0865},
	OPT_URL = {https://ieeexplore.ieee.org/abstract/document/10620070},
	doi = {10.1109/TCST.2024.3426300},
	abstract = {Reinforcement learning (RL) is a widely used control technique that finds an optimal policy using the feedback of its actions. The search for the optimal policy requires that the system explores a broad region of the state space. This search puts at risk the safe operation, since some of the explored regions might be near the physical system limits. Implementing learning methods in industrial applications is limited because of its uncertain behavior when finding an optimal policy. This work proposes an RL control algorithm with a filter that supervises the safety of the exploration based on a nominal model. The performance of this safety filter is increased by modeling the uncertainty with a Gaussian process (GP) regression. This method is applied to the optimization of the management of a water distribution network (WDN) with an elevated reservoir; the management objectives are to regulate the tank filling while maintaining an adequate water turnover. The proposed method is validated in a laboratory setup that emulates the hydraulic features of a WDN.},
	number = {6},
	OPT_URLdate = {2024-12-28},
	journal = {IEEE Transactions on Control Systems Technology},
	author = {Val Ledesma, Jorge and Wisniewski, Rafał and Kallesøe, Carsten S. and Tsouvalas, Agisilaos},
	month = nov,
	year = {2024},
	keywords = {Optimization, Distribution networks, Reinforcement learning, Safety, Feedback, Gaussian process (GP), Gaussian processes, reinforcement learning (RL), Reservoirs, safety, Subspace constraints, water age, water distribution network (WDN), Water monitoring, water quality, Water quality},
	pages = {2332--2343},
	file = {Val Ledesma et al. - 2024 - Water Age Control for Water Distribution Networks .pdf:/Users/paulstahlhofen/Zotero/storage/ZGHSFKKK/Val Ledesma et al. - 2024 - Water Age Control for Water Distribution Networks .pdf:application/pdf},
}

@article{hu_deep_2020,
	title = {Deep reinforcement learning based valve scheduling for pollution isolation in water distribution network},
	volume = {17},
	issn = {1551-0018},
	OPT_URL = {http://www.aimspress.com/article/10.3934/mbe.2020006},
	doi = {10.3934/mbe.2020006},
	language = {en},
	number = {1},
	OPT_URLdate = {2025-01-06},
	journal = {Mathematical Biosciences and Engineering},
	author = {Hu, Chengyu and Cai, Junyi and Zeng, Deze and Yan, Xuesong and Gong, Wenyin and Wang, Ling and {1 Department of Computer Science, China university of geosciences, Wuhan, China} and {2 Department of Automation Control, Tsinghua University, Beijing, China}},
	year = {2020},
	pages = {105--121},
	file = {Deep reinforcement learning based valve scheduling.pdf:/Users/paulstahlhofen/Zotero/storage/89X7W2NV/Deep reinforcement learning based valve scheduling.pdf:application/pdf},
}

@article{negm_deep_2024,
	title = {Deep reinforcement learning challenges and opportunities for urban water systems},
	volume = {253},
	issn = {00431354},
	OPT_URL = {https://linkinghub.elsevier.com/retrieve/pii/S0043135424000459},
	doi = {10.1016/j.watres.2024.121145},
	language = {en},
	OPT_URLdate = {2025-01-06},
	journal = {Water Research},
	author = {Negm, Ahmed and Ma, Xiandong and Aggidis, George},
	month = apr,
	year = {2024},
	pages = {121145},
	file = {Negm et al. - 2024 - Deep reinforcement learning challenges and opportu.pdf:/Users/paulstahlhofen/Zotero/storage/KYFP4CBT/Negm et al. - 2024 - Deep reinforcement learning challenges and opportu.pdf:application/pdf},
}

@misc{beferull-lozano_energy_2023,
	title = {Energy efficient online control of a water distribution network based on {Deep} {Reinforcement} {Learning}},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	OPT_URL = {https://www.techrxiv.org/doi/full/10.36227/techrxiv.23896902.v1},
	doi = {10.36227/techrxiv.23896902.v1},
	abstract = {{\textless}p{\textgreater}Data is obtained via Deep Reinforcement Learning Environment{\textless}/p{\textgreater}},
	OPT_URLdate = {2025-01-06},
	author = {Beferull-Lozano, Baltasar and Bhardwaj, Jyotirmoy and Liltvedt, Helge},
	month = aug,
	year = {2023},
	file = {Beferull-Lozano et al. - 2023 - Energy efficient online control of a water distrib.pdf:/Users/paulstahlhofen/Zotero/storage/CWHTTUZQ/Beferull-Lozano et al. - 2023 - Energy efficient online control of a water distrib.pdf:application/pdf},
}

@article{hu_multi-agent_2023,
	title = {Multi-agent reinforcement learning framework for real-time scheduling of pump and valve in water distribution networks},
	volume = {23},
	issn = {1606-9749, 1607-0798},
	OPT_URL = {https://iwaponline.com/ws/article/23/7/2833/96109/Multi-agent-reinforcement-learning-framework-for},
	doi = {10.2166/ws.2023.163},
	abstract = {Abstract

            With energy and water resources shortages, the energy and water resources managements of water distribution networks (WDNs) have become increasingly important. However, achieving real-time scheduling of pump and valve in dynamic environments remains challenging. Thus, this study proposes a multi-agent reinforcement learning scheduling framework to address the uncertainty of water demand in WDNs. First, we constructed a WDN environment and modelled the scheduling problem as a Markov decision process. Second, a multi-agent deep deterministic policy gradient (MADDPG) method was used to determine the strategy of the fully cooperative multi-agent task. Moreover, the impacts of energy and water loss costs on the scheduling strategy were explored. Finally, the results were compared with those of a genetic algorithm (GA), particle swarm optimisation (PSO), and differential evolution (DE) to verify the performance and robustness of the proposed model. The results show that water loss dominates the scheduling process, and the scheduling solutions for minimising water loss and energy costs are mainly affected by the demand pattern of consumers rather than the energy tariff. The proposed MADDPG model outperforms the GA, PSO, and DE models, achieving a significantly faster solution, which is advantageous for practical applications.},
	language = {en},
	number = {7},
	OPT_URLdate = {2025-01-07},
	journal = {Water Supply},
	author = {Hu, Shiyuan and Gao, Jinliang and Zhong, Dan},
	month = jul,
	year = {2023},
	pages = {2833--2846},
	file = {Hu et al. - 2023 - Multi-agent reinforcement learning framework for r.pdf:/Users/paulstahlhofen/Zotero/storage/3VRNPMKY/Hu et al. - 2023 - Multi-agent reinforcement learning framework for r.pdf:application/pdf},
}

@inproceedings{belfadil_drl-epanet_2022,
	title = {{DRL}-{Epanet}: {Deep} reinforcement learning for optimal control at scale in {Water} {Distribution} {Systems}},
	OPT_URL = {https://openreview.net/pdf?id=fTVRewWKuLI},
	OPT_URLdate = {2025-01-09},
	booktitle = {Deep {Reinforcement} {Learning} {Workshop}},
	author = {Belfadil, Anas and Modesto, David and Martin Hernandez, Jose Antonio},
	year = {2022},
	file = {Belfadil et al. - 2022 - DRL-Epanet Deep reinforcement learning for optima.pdf:/Users/paulstahlhofen/Zotero/storage/9FKA4QD7/Belfadil et al. - 2022 - DRL-Epanet Deep reinforcement learning for optima.pdf:application/pdf},
}

@article{perelman_data-enabled_2024,
	title = {Data-{Enabled} {Predictive} {Control} for {Optimal} {Pressure} {Management}},
	volume = {69},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2673-4591},
	OPT_URL = {https://www.mdpi.com/2673-4591/69/1/5},
	doi = {10.3390/engproc2024069005},
	abstract = {Recent developments in control theory coupled with the growing availability of real-time data have paved the way for improved data-driven control methodologies. This study explores the application of a data-enabled predictive control (DeePC) algorithm to optimize the operation of water distribution systems (WDS). WDSs are characterized by inherent uncertainties and complex nonlinear dynamics. Hence, classic control strategies that involve physical model-based methods are often hard to implement and infeasible to scale. The DeePC method suggests a paradigm shift by utilizing a data-driven approach. This method employs real-time data to dynamically learn an unknown system’s behavior. It utilizes a finite set of input–output samples (control settings, and measured data) to derive optimal policies, effectively bypassing the need for an explicit mathematical model of the system. In this study, DeePC is applied to a pressure management case study and demonstrates superior performance compared to standard control strategies.},
	language = {en},
	number = {1},
	OPT_URLdate = {2025-01-27},
	journal = {Engineering Proceedings},
	author = {Perelman, Gal and Ostfeld, Avi},
	year = {2024},
	keywords = {data-driven, uncertainty, water distribution systems, predictive control, real-time},
	pages = {5},
}

@article{fiedler_economic_2020,
	series = {21st {IFAC} {World} {Congress}},
	title = {Economic nonlinear predictive control of water distribution networks based on surrogate modeling and automatic clustering},
	volume = {53},
	issn = {2405-8963},
	OPT_URL = {https://www.sciencedirect.com/science/article/pii/S2405896320311174},
	doi = {10.1016/j.ifacol.2020.12.793},
	abstract = {The operation of large-scale water distribution networks (WDNs) is a complex control task due to the size of the problem, the need to consider key operational, quality and safety-related constraints as well as because of the presence of uncertainties. An efficient operation of WDNs can lead to considerable reduction in the energy used to distribute the required amounts of water, leading to significant economic savings. Many model predictive control (MPC) schemes have been proposed in the literature to tackle this control problem. However, finding a control-oriented model that can be used in an optimization framework, which captures nonlinear behavior of the water network and is of a manageable size is a very important challenge faced in practice. We propose the use of a data-based automatic clustering method that clusters similar nodes of the network to reduce the model size and then learn a deep-learning based model of the clustered network. The learned model is used within an economic nonlinear MPC framework. The proposed method leads to a flexible scheme for economic robust nonlinear MPC of large WDNs that can be solved in real time, leads to significant energy savings and is robust to uncertain water demands. The potential of the proposed approach is illustrated by simulation results of a benchmark WDN model.},
	number = {2},
	OPT_URLdate = {2025-02-05},
	journal = {IFAC-PapersOnLine},
	author = {Fiedler, Felix and Cominola, Andrea and Lucia, Sergio},
	month = jan,
	year = {2020},
	keywords = {water distribution networks, machine learning, model predictive control},
	pages = {16636--16643},
}

@article{pei_real-time_2025,
	title = {Real-{Time} {Pump} {Scheduling} in {Water} {Distribution} {Networks} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {151},
	issn = {0733-9496, 1943-5452},
	OPT_URL = {https://ascelibrary.org/doi/10.1061/JWRMD5.WRENG-6476},
	doi = {10.1061/JWRMD5.WRENG-6476},
	language = {en},
	number = {6},
	OPT_URLdate = {2025-03-27},
	journal = {Journal of Water Resources Planning and Management},
	author = {Pei, Shengwei and Hoang, Lan and Fu, Guangtao and Butler, David},
	month = jun,
	year = {2025},
	pages = {04025012},
	file = {Pei et al. - 2025 - Real-Time Pump Scheduling in Water Distribution Ne.pdf:/Users/paulstahlhofen/Zotero/storage/PTVMCALQ/Pei et al. - 2025 - Real-Time Pump Scheduling in Water Distribution Ne.pdf:application/pdf},
}

@article{mala-jetmarova_lost_2017,
	title = {Lost in optimisation of water distribution systems? {A} literature review of system operation},
	volume = {93},
	issn = {13648152},
	shorttitle = {Lost in optimisation of water distribution systems?},
	OPT_URL = {https://linkinghub.elsevier.com/retrieve/pii/S1364815216307769},
	doi = {10.1016/j.envsoft.2017.02.009},
	language = {en},
	OPT_URLdate = {2025-04-08},
	journal = {Environmental Modelling \& Software},
	author = {Mala-Jetmarova, Helena and Sultanova, Nargiz and Savic, Dragan},
	month = jul,
	year = {2017},
	pages = {209--254},
	file = {main.pdf:/Users/paulstahlhofen/Zotero/storage/IVIMQIH5/main.pdf:application/pdf},
}

@article{ostfeld_ant_2008,
	title = {Ant {Colony} {Optimization} for {Least}-{Cost} {Design} and {Operation} of {Pumping} {Water} {Distribution} {Systems}},
	volume = {134},
	issn = {0733-9496, 1943-5452},
	OPT_URL = {https://ascelibrary.org/doi/10.1061/%28ASCE%290733-9496%282008%29134%3A2%28107%29},
	doi = {10.1061/(ASCE)0733-9496(2008)134:2(107)},
	language = {en},
	number = {2},
	OPT_URLdate = {2024-07-31},
	journal = {Journal of Water Resources Planning and Management},
	author = {Ostfeld, Avi and Tubaltzev, Ariel},
	month = mar,
	year = {2008},
	pages = {107--118},
	file = {Ostfeld und Tubaltzev - 2008 - Ant Colony Optimization for Least-Cost Design and .pdf:/Users/paulstahlhofen/Zotero/storage/C6MVAAKM/Ostfeld und Tubaltzev - 2008 - Ant Colony Optimization for Least-Cost Design and .pdf:application/pdf},
}

@misc{noauthor_suttonbartoiprlbook2ndedpdf_nodate,
	title = {{SuttonBartoIPRLBook2ndEd}.pdf},
	file = {SuttonBartoIPRLBook2ndEd.pdf:/Users/paulstahlhofen/Zotero/storage/L8C9LN5U/SuttonBartoIPRLBook2ndEd.pdf:application/pdf},
}

@misc{noauthor_moos_2022_robust-reinforcement-learning--review--foundations-and-recent-advancespdf_nodate,
	title = {Moos\_2022\_Robust-{Reinforcement}-{Learning}-{A}-{Review}-of-{Foundations}-and-{Recent}-{Advances}.pdf},
	file = {Moos_2022_Robust-Reinforcement-Learning-A-Review-of-Foundations-and-Recent-Advances.pdf:/Users/paulstahlhofen/Zotero/storage/G4AENAZ9/Moos_2022_Robust-Reinforcement-Learning-A-Review-of-Foundations-and-Recent-Advances.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	address = {Camebridge, Massachusetts},
	edition = {2},
	title = {Reinforcement {Learning} - {An} {Introduction}},
	publisher = {The MIT Press},
	author = {Sutton, Richard and Barto, Andrew},
	year = {2018},
	file = {Sutton und Barto - 2018 - Reinforcement Learning - An Introduction.pdf:/Users/paulstahlhofen/Zotero/storage/MRHBPZEE/Sutton und Barto - 2018 - Reinforcement Learning - An Introduction.pdf:application/pdf},
}

@misc{zhang_robust_2021,
	title = {Robust {Reinforcement} {Learning} on {State} {Observations} with {Learned} {Optimal} {Adversary}},
	OPT_URL = {http://arxiv.org/abs/2101.08452},
	abstract = {We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA\_robust\_RL.},
	OPT_URLdate = {2024-06-04},
	author = {Zhang, Huan and Chen, Hongge and Boning, Duane and Hsieh, Cho-Jui},
	month = jan,
	year = {2021},
	note = {arXiv:2101.08452 [cs, stat]},
	file = {Zhang et al. - 2021 - Robust Reinforcement Learning on State Observations with Learned Optimal Adversary.pdf:/Users/paulstahlhofen/Zotero/storage/DKXZF3SD/Zhang et al. - 2021 - Robust Reinforcement Learning on State Observations with Learned Optimal Adversary.pdf:application/pdf},
}

@inproceedings{zhang_robust_2020,
	title = {Robust {Deep} {Reinforcement} {Learning} against {Adversarial} {Perturbations} on {State} {Observations}},
	author = {Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui},
	year = {2020},
	file = {Zhang_2020.pdf:/Users/paulstahlhofen/Zotero/storage/PX9M4LXH/f0eb6568ea114ba6e293f903c34d7488-Paper.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	OPT_URL = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	OPT_URLdate = {2024-06-13},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	file = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/Users/paulstahlhofen/Zotero/storage/RST84PGT/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	OPT_URL = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	OPT_URLdate = {2024-11-14},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv:1312.5602 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/KFGLNR6Z/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/WZ8HKQCR/1312.html:text/html},
}

@inproceedings{van_hasselt_reinforcement_2007,
	address = {Honolulu, HI, USA},
	title = {Reinforcement {Learning} in {Continuous} {Action} {Spaces}},
	isbn = {978-1-4244-0706-4},
	OPT_URL = {http://ieeexplore.ieee.org/document/4220844/},
	doi = {10.1109/ADPRL.2007.368199},
	OPT_URLdate = {2024-11-18},
	publisher = {IEEE},
	author = {Van Hasselt, Hado and Wiering, Marco A.},
	month = apr,
	year = {2007},
	pages = {272--279},
	file = {reinforcement_learning_in_continuous_action_spaces.pdf:/Users/paulstahlhofen/Zotero/storage/X787KQPA/reinforcement_learning_in_continuous_action_spaces.pdf:application/pdf},
}

@inproceedings{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	volume = {80},
	shorttitle = {Soft {Actor}-{Critic}},
	OPT_URL = {https://proceedings.mlr.press/v80/haarnoja18b},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	OPT_URLdate = {2025-02-24},
	booktitle = {{PMLR}},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	pages = {1861--1870},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/Q3PVBAVB/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/GHNQFSDL/1801.html:text/html},
}

@article{ha_world_2018,
	title = {World {Models}},
	OPT_URL = {http://arxiv.org/abs/1803.10122},
	doi = {10.5281/zenodo.1207631},
	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
	OPT_URLdate = {2024-11-20},
	author = {Ha, David and Schmidhuber, Jürgen},
	month = mar,
	year = {2018},
	note = {arXiv:1803.10122 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/NGVHAQCJ/Ha und Schmidhuber - 2018 - World Models.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/P9B7GKVE/1803.html:text/html},
}

@misc{lahire_actor_2021,
	title = {Actor {Loss} of {Soft} {Actor} {Critic} {Explained}},
	OPT_URL = {http://arxiv.org/abs/2112.15568},
	doi = {10.48550/arXiv.2112.15568},
	abstract = {This technical report is devoted to explaining how the actor loss of soft actor critic is obtained, as well as the associated gradient estimate. It gives the necessary mathematical background to derive all the presented equations, from the theoretical actor loss to the one implemented in practice. This necessitates a comparison of the reparameterization trick used in soft actor critic with the nabla log trick, which leads to open questions regarding the most efficient method to use.},
	OPT_URLdate = {2024-11-27},
	publisher = {arXiv},
	author = {Lahire, Thibault},
	month = dec,
	year = {2021},
	note = {arXiv:2112.15568 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/57I7TSXV/Lahire - 2021 - Actor Loss of Soft Actor Critic Explained.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/SMQTM9NF/2112.html:text/html},
}

@misc{raffin_smooth_2021,
	title = {Smooth {Exploration} for {Robotic} {Reinforcement} {Learning}},
	OPT_URL = {http://arxiv.org/abs/2005.05719},
	doi = {10.48550/arXiv.2005.05719},
	abstract = {Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.},
	OPT_URLdate = {2024-12-04},
	publisher = {arXiv},
	author = {Raffin, Antonin and Kober, Jens and Stulp, Freek},
	month = jun,
	year = {2021},
	note = {arXiv:2005.05719 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/K6Y4728Q/Raffin et al. - 2021 - Smooth Exploration for Robotic Reinforcement Learn.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/GCQ5JCXF/2005.html:text/html},
}

@misc{wang_meta-sac_2020,
	title = {Meta-{SAC}: {Auto}-tune the {Entropy} {Temperature} of {Soft} {Actor}-{Critic} via {Metagradient}},
	shorttitle = {Meta-{SAC}},
	OPT_URL = {http://arxiv.org/abs/2007.01932},
	doi = {10.48550/arXiv.2007.01932},
	abstract = {Exploration-exploitation dilemma has long been a crucial issue in reinforcement learning. In this paper, we propose a new approach to automatically balance between these two. Our method is built upon the Soft Actor-Critic (SAC) algorithm, which uses an "entropy temperature" that balances the original task reward and the policy entropy, and hence controls the trade-off between exploitation and exploration. It is empirically shown that SAC is very sensitive to this hyperparameter, and the follow-up work (SAC-v2), which uses constrained optimization for automatic adjustment, has some limitations. The core of our method, namely Meta-SAC, is to use metagradient along with a novel meta objective to automatically tune the entropy temperature in SAC. We show that Meta-SAC achieves promising performances on several of the Mujoco benchmarking tasks, and outperforms SAC-v2 over 10\% in one of the most challenging tasks, humanoid-v2.},
	OPT_URLdate = {2024-12-05},
	publisher = {arXiv},
	author = {Wang, Yufei and Ni, Tianwei},
	month = jul,
	year = {2020},
	note = {arXiv:2007.01932 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/P44PJYJL/Wang und Ni - 2020 - Meta-SAC Auto-tune the Entropy Temperature of Sof.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/EZV9EGX9/2007.html:text/html},
}

@misc{haarnoja_soft_2019,
	title = {Soft {Actor}-{Critic} {Algorithms} and {Applications}},
	OPT_URL = {http://arxiv.org/abs/1812.05905},
	doi = {10.48550/arXiv.1812.05905},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
	OPT_URLdate = {2024-12-05},
	publisher = {arXiv},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
	month = jan,
	year = {2019},
	note = {arXiv:1812.05905 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/467GTJXI/Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/9N6Q4JJW/1812.html:text/html},
}

@misc{murphy_reinforcement_2024,
	title = {Reinforcement {Learning}: {An} {Overview}},
	shorttitle = {Reinforcement {Learning}},
	OPT_URL = {http://arxiv.org/abs/2412.05265},
	doi = {10.48550/arXiv.2412.05265},
	abstract = {This manuscript gives a big-picture, up-to-date overview of the field of (deep) reinforcement learning and sequential decision making, covering value-based RL, policy-gradient methods, model-based methods, and various other topics (including a very brief discussion of RL+LLMs).},
	OPT_URLdate = {2024-12-09},
	publisher = {arXiv},
	author = {Murphy, Kevin},
	month = dec,
	year = {2024},
	note = {arXiv:2412.05265 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/paulstahlhofen/Zotero/storage/93L7BZTI/Murphy - 2024 - Reinforcement Learning An Overview.pdf:application/pdf;Snapshot:/Users/paulstahlhofen/Zotero/storage/LY4XBA75/2412.html:text/html},
}

@misc{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	OPT_URL = {http://arxiv.org/abs/2005.01643},
	doi = {10.48550/arXiv.2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	OPT_URLdate = {2024-12-11},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01643 [cs]},
	file = {Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, and Perspectives on Open Problems.pdf:/Users/paulstahlhofen/Zotero/storage/DMFTERBE/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, and Perspectives on Open Problems.pdf:application/pdf},
}

@article{baran_multi-objective_2005,
	title = {Multi-objective pump scheduling optimisation using evolutionary strategies},
	volume = {36},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09659978},
	OPT_URL = {https://linkinghub.elsevier.com/retrieve/pii/S096599780400078X},
	doi = {10.1016/j.advengsoft.2004.03.012},
	language = {en},
	number = {1},
	OPT_URLdate = {2025-04-08},
	journal = {Advances in Engineering Software},
	author = {Barán, Benjamı́n and Von Lücken, Christian and Sotelo, Aldo},
	month = jan,
	year = {2005},
	pages = {39--47},
	file = {main.pdf:/Users/paulstahlhofen/Zotero/storage/2ZBC42RW/main.pdf:application/pdf},
}

@article{giustolisi_operational_2013,
	title = {Operational {Optimization}: {Water} {Losses} versus {Energy} {Costs}},
	volume = {139},
	issn = {0733-9429, 1943-7900},
	shorttitle = {Operational {Optimization}},
	OPT_URL = {https://ascelibrary.org/doi/10.1061/%28ASCE%29HY.1943-7900.0000681},
	doi = {10.1061/(ASCE)HY.1943-7900.0000681},
	language = {en},
	number = {4},
	OPT_URLdate = {2025-04-09},
	journal = {Journal of Hydraulic Engineering},
	author = {Giustolisi, O. and Laucelli, D. and Berardi, L.},
	month = apr,
	year = {2013},
	pages = {410--423},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	copyright = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	OPT_URL = {http://link.springer.com/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	language = {en},
	number = {3-4},
	OPT_URLdate = {2025-04-09},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	pages = {279--292},
	file = {Volltext:/Users/paulstahlhofen/Zotero/storage/GRC2WBWX/Watkins und Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{walski_battle_1987,
	title = {Battle of the {Network} {Models}: {Epilogue}},
	volume = {113},
	issn = {0733-9496, 1943-5452},
	shorttitle = {Battle of the {Network} {Models}},
	OPT_URL = {https://ascelibrary.org/doi/10.1061/%28ASCE%290733-9496%281987%29113%3A2%28191%29},
	doi = {10.1061/(ASCE)0733-9496(1987)113:2(191)},
	language = {en},
	number = {2},
	OPT_URLdate = {2025-04-14},
	journal = {Journal of Water Resources Planning and Management},
	author = {Walski, Thomas M. and Brill, E. Downey and Gessler, Johannes and Goulter, Ian C. and Jeppson, Roland M. and Lansey, Kevin and Lee, Han‐Lin and Liebman, Jon C. and Mays, Larry and Morgan, David R. and Ormsbee, Lindell},
	month = mar,
	year = {1987},
	pages = {191--203},
}

@article{ghorbanian_pressure_2016,
	title = {Pressure {Standards} in {Water} {Distribution} {Systems}: {Reflection} on {Current} {Practice} with {Consideration} of {Some} {Unresolved} {Issues}},
	volume = {142},
	issn = {0733-9496, 1943-5452},
	shorttitle = {Pressure {Standards} in {Water} {Distribution} {Systems}},
	OPT_URL = {https://ascelibrary.org/doi/10.1061/%28ASCE%29WR.1943-5452.0000665},
	doi = {10.1061/(ASCE)WR.1943-5452.0000665},
	abstract = {Pressure standards assist in the design of water distribution systems and the assessment of their performance. Although exact thresholds are sometimes rather vague, unusually high and low pressures are widely understood to increase costs and put systems at risk from events like pipe bursts at the high-pressure end to the risk of contaminant intrusion or poor firefighting conditions at the low-pressure end. Interestingly, because the definition of what conditions constitute acceptable pressures differs around the world, a delivery pressure might be considered acceptable in some regions and unacceptable in others. But if a wider range of system conditions is considered, including transient events, an interesting question arises as to what exactly the standards might mean and how violations should be evaluated. Specifically, what kinds of pressure transgressions are most crucial to system performance and economics and what kinds are merely inconvenient? Certainly the issue of evaluating consequences is relevant, but it is also complex because some consequences are not easily attributed to specific system conditions. As this paper considers, the frequency, duration, and intensity of the pressure violation are all relevant, but so is the vulnerability of the system to those violations. Few of these issues have yet received adequate attention, but it is to raising such a discussion that this paper is aimed. There is no doubt that pressure standards can help to assess system performance and to trigger system evolution (i.e., operational and capital investments). But if the criteria themselves, and the means to evaluate them, are too vague, so will be the corrective outcomes. DOI: 10.1061/(ASCE)WR.1943-5452.0000665. © 2016 American Society of Civil Engineers.},
	language = {en},
	number = {8},
	OPT_URLdate = {2024-06-24},
	journal = {Journal of Water Resources Planning and Management},
	author = {Ghorbanian, Vali and Karney, Bryan and Guo, Yiping},
	month = aug,
	year = {2016},
	pages = {04016023},
	file = {Ghorbanian et al. - 2016 - Pressure Standards in Water Distribution Systems .pdf:/Users/paulstahlhofen/Zotero/storage/RERL4I6V/Ghorbanian et al. - 2016 - Pressure Standards in Water Distribution Systems .pdf:application/pdf},
}

@inproceedings{mckee_review_2011,
	title = {A review of major centrifugal pump failure modes with application to the water supply and sewerage industries},
	booktitle = {{ICOMS} asset management proceedings},
	author = {McKee, Kristoffer and Forbes, Gareth and Mazhar, Muhammad Ilyas and Entwistle, Rodney and Howard, Ian},
	year = {2011},
}

@article{artelt_epyt-flow_2024,
	title = {{EPyT}-{Flow}: {A} {Toolkit} for {Generating} {Water} {DistributionNetwork} {Data}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2475-9066},
	shorttitle = {{EPyT}-{Flow}},
	OPT_URL = {https://joss.theoj.org/papers/10.21105/joss.07104},
	doi = {10.21105/joss.07104},
	number = {103},
	OPT_URLdate = {2025-04-15},
	journal = {Journal of Open Source Software},
	author = {Artelt, André and Kyriakou, Marios S. and Vrachimis, Stelios G. and Eliades, Demetrios G. and Hammer, Barbara and Polycarpou, Marios M.},
	month = nov,
	year = {2024},
	pages = {7104},
}

@article{raffin_stable_2021,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  OPT_URL     = {http://jmlr.org/papers/v22/20-1364.html}
}

@book{rossman_epanet_2020,
	author = {Rossman, Lewis A. and Woo, Hyoungmin and Tryby, Micheal and Shang, Feng and Janke, Robert and Haxton, Terranna},
	title = {EPANET 2.2. User Manual},
	publisher = {U.S. Environmental Protection Agency},
	year = {2020},
	address = {Washington D.C.},
	issn = {EPA/600/R-20/133}
}